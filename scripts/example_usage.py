#!/usr/bin/env python3
"""
Exemplo de uso do libervia-ia-Core v1
Demonstra as principais funcionalidades do modelo
"""

import sys
import time
from pathlib import Path

import torch
import torch.nn.functional as F

# Adicionar o package ao path
sys.path.insert(0, str(Path(__file__).parent.parent / "packages" / "core"))

from libervia_core import (
    LiberviaConfig,
    LiberviaForCausalLM,
    create_model,
    create_config,
    print_model_info,
    check_system_compatibility,
)


def example_1_basic_usage():
    """Exemplo 1: Uso b√°sico do modelo."""
    print("üìù Exemplo 1: Uso B√°sico")
    print("-" * 30)
    
    # Criar modelo rapidamente
    model = create_model("2b")
    
    print(f"Modelo criado: {model.config.model_name}")
    print(f"Par√¢metros: {model.num_parameters():,}")
    print(f"Mem√≥ria: {model.get_memory_footprint():.1f} MB")
    
    # Teste b√°sico
    input_ids = torch.randint(0, 1000, (1, 5))
    
    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs['logits']
    
    print(f"Input shape: {input_ids.shape}")
    print(f"Output shape: {logits.shape}")
    print("‚úÖ Uso b√°sico funcionando!")


def example_2_custom_config():
    """Exemplo 2: Configura√ß√£o customizada."""
    print("\nüìù Exemplo 2: Configura√ß√£o Customizada")
    print("-" * 40)
    
    # Configura√ß√£o personalizada
    config = create_config("2b", 
        max_sequence_length=8192,
        torch_dtype="bfloat16",
        use_flash_attention=True,
        dropout=0.1
    )
    
    print(f"Configura√ß√£o customizada:")
    print(f"  Sequ√™ncia m√°xima: {config.max_sequence_length}")
    print(f"  Tipo de dados: {config.torch_dtype}")
    print(f"  Flash Attention: {config.use_flash_attention}")
    print(f"  Dropout: {config.dropout}")
    
    # Criar modelo com config customizada
    model = LiberviaForCausalLM(config)
    print(f"‚úÖ Modelo criado com config customizada!")


def example_3_inference_optimization():
    """Exemplo 3: Otimiza√ß√µes para infer√™ncia."""
    print("\nüìù Exemplo 3: Otimiza√ß√µes para Infer√™ncia")
    print("-" * 45)
    
    # Configura√ß√£o otimizada para infer√™ncia
    config = create_config("2b",
        dropout=0.0,              # Desligar dropout
        use_kv_cache=True,        # Usar KV cache
        torch_dtype="float16"     # Menor precis√£o
    )
    
    model = LiberviaForCausalLM(config)
    model.eval()  # Modo infer√™ncia
    
    print("üéØ Comparando performance com e sem KV cache...")
    
    # Prompt inicial
    prompt_ids = torch.randint(0, config.vocab_size, (1, 10))
    
    # Sem cache (regenerar tudo)
    start_time = time.time()
    with torch.no_grad():
        for i in range(5):
            # Simular regenera√ß√£o completa
            current_ids = prompt_ids[:, :i+5]
            outputs = model(current_ids)
    no_cache_time = time.time() - start_time
    
    # Com cache
    start_time = time.time()
    past_key_values = None
    with torch.no_grad():
        for i in range(5):
            if i == 0:
                # Primeira passada
                current_ids = prompt_ids
                outputs = model(current_ids, use_cache=True)
                past_key_values = outputs['past_key_values']
            else:
                # Pr√≥ximos tokens
                next_token = torch.randint(0, config.vocab_size, (1, 1))
                outputs = model(next_token, past_key_values=past_key_values, use_cache=True)
                past_key_values = outputs['past_key_values']
    
    cache_time = time.time() - start_time
    
    print(f"Sem cache: {no_cache_time:.4f}s")
    print(f"Com cache: {cache_time:.4f}s") 
    print(f"Speedup: {no_cache_time/cache_time:.1f}x")
    print("‚úÖ KV cache acelera infer√™ncia!")


def example_4_text_generation():
    """Exemplo 4: Gera√ß√£o de texto simples."""
    print("\nüìù Exemplo 4: Gera√ß√£o de Texto")
    print("-" * 35)
    
    # Modelo pequeno para gera√ß√£o r√°pida
    config = create_config("2b")
    config = config.update(d_model=256, n_layers=4, vocab_size=1000)
    
    model = LiberviaForCausalLM(config)
    model.eval()
    
    print("ü§ñ Gerando sequ√™ncia de tokens...")
    
    # Par√¢metros de gera√ß√£o
    max_new_tokens = 15
    temperature = 0.8
    top_p = 0.9
    
    # Prompt inicial (tokens aleat√≥rios como exemplo)
    generated_ids = torch.randint(0, config.vocab_size, (1, 5))
    
    print(f"Prompt inicial: {generated_ids.tolist()[0]}")
    
    # Gera√ß√£o token por token
    with torch.no_grad():
        for i in range(max_new_tokens):
            # Forward pass
            outputs = model(generated_ids)
            logits = outputs['logits']
            
            # Logits do √∫ltimo token
            next_token_logits = logits[0, -1, :] / temperature
            
            # Top-p sampling
            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
            
            # Remover tokens com probabilidade cumulativa > top_p
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
            sorted_indices_to_remove[0] = 0
            
            indices_to_remove = sorted_indices[sorted_indices_to_remove]
            next_token_logits[indices_to_remove] = float('-inf')
            
            # Sample
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # Adicionar √† sequ√™ncia
            generated_ids = torch.cat([generated_ids, next_token.unsqueeze(0)], dim=1)
    
    print(f"Sequ√™ncia final: {generated_ids.tolist()[0]}")
    print(f"Tokens gerados: {max_new_tokens}")
    print("‚úÖ Gera√ß√£o de texto funcionando!")


def example_5_model_comparison():
    """Exemplo 5: Compara√ß√£o entre tamanhos."""
    print("\nüìù Exemplo 5: Compara√ß√£o de Modelos")
    print("-" * 40)
    
    model_sizes = ["2b", "4b", "7b"]
    
    print(f"{'Modelo':<10} {'Par√¢metros':<15} {'Mem√≥ria (MB)':<12} {'Tempo (ms)':<12}")
    print("-" * 55)
    
    for size in model_sizes:
        try:
            # Configs reduzidas para teste r√°pido
            config = create_config(size)
            config = config.update(
                d_model=min(config.d_model, 512),
                n_layers=min(config.n_layers, 2)
            )
            
            model = LiberviaForCausalLM(config)
            
            # Teste de performance
            input_ids = torch.randint(0, config.vocab_size, (1, 10))
            
            start_time = time.time()
            with torch.no_grad():
                outputs = model(input_ids)
            inference_time = (time.time() - start_time) * 1000  # ms
            
            params = model.num_parameters()
            memory = model.get_memory_footprint()
            
            print(f"{size.upper():<10} {params:<15,} {memory:<12.1f} {inference_time:<12.1f}")
            
        except Exception as e:
            print(f"{size.upper():<10} {'ERRO':<15} {str(e)[:20]:<12}")
    
    print("‚úÖ Compara√ß√£o de modelos conclu√≠da!")


def example_6_save_and_load():
    """Exemplo 6: Salvar e carregar configura√ß√£o."""
    print("\nüìù Exemplo 6: Salvar e Carregar")
    print("-" * 35)
    
    # Criar configura√ß√£o
    config = create_config("2b", max_sequence_length=4096)
    
    # Salvar configura√ß√£o
    save_path = Path("./temp_config")
    save_path.mkdir(exist_ok=True)
    
    config.save_pretrained(save_path)
    print(f"‚úÖ Configura√ß√£o salva em: {save_path}")
    
    # Carregar configura√ß√£o
    loaded_config = LiberviaConfig.from_pretrained(save_path)
    print(f"‚úÖ Configura√ß√£o carregada: {loaded_config.model_name}")
    
    # Verificar se s√£o iguais
    assert config.d_model == loaded_config.d_model
    assert config.n_layers == loaded_config.n_layers
    print("‚úÖ Configura√ß√µes s√£o id√™nticas!")
    
    # Limpeza
    import shutil
    shutil.rmtree(save_path)
    print("‚úÖ Arquivos tempor√°rios removidos")


def example_7_cuda_usage():
    """Exemplo 7: Uso com CUDA (se dispon√≠vel)."""
    print("\nüìù Exemplo 7: Uso com CUDA")
    print("-" * 30)
    
    if not torch.cuda.is_available():
        print("‚ö†Ô∏è  CUDA n√£o dispon√≠vel, pulando exemplo")
        return
    
    print(f"üéÆ GPU dispon√≠vel: {torch.cuda.get_device_name()}")
    
    # Modelo para CUDA
    config = create_config("2b")
    config = config.update(d_model=256, n_layers=2, torch_dtype="float16")
    
    model = LiberviaForCausalLM(config)
    model = model.cuda().half()  # Mover para GPU e converter para FP16
    
    input_ids = torch.randint(0, config.vocab_size, (1, 10)).cuda()
    
    # Benchmark CPU vs GPU
    model_cpu = LiberviaForCausalLM(config).eval()
    input_cpu = torch.randint(0, config.vocab_size, (1, 10))
    
    # CPU
    start_time = time.time()
    with torch.no_grad():
        outputs_cpu = model_cpu(input_cpu)
    cpu_time = time.time() - start_time
    
    # GPU
    start_time = time.time()
    with torch.no_grad():
        outputs_gpu = model(input_ids)
    gpu_time = time.time() - start_time
    
    print(f"CPU time: {cpu_time:.4f}s")
    print(f"GPU time: {gpu_time:.4f}s")
    print(f"GPU speedup: {cpu_time/gpu_time:.1f}x")
    print(f"GPU memory: {torch.cuda.memory_allocated()/1024**2:.1f} MB")
    print("‚úÖ CUDA funcionando!")


def main():
    """Fun√ß√£o principal - executa todos os exemplos."""
    print("üåü libervia-ia-Core v1 - Exemplos de Uso")
    print("=" * 50)
    
    # Informa√ß√µes do sistema
    system_info = check_system_compatibility()
    print(f"Sistema: Python {system_info['python_version']}, PyTorch {system_info['torch_version']}")
    print(f"CUDA: {system_info['cuda_available']}")
    
    # Lista de exemplos
    examples = [
        example_1_basic_usage,
        example_2_custom_config,
        example_3_inference_optimization,
        example_4_text_generation,
        example_5_model_comparison,
        example_6_save_and_load,
        example_7_cuda_usage,
    ]
    
    # Executar exemplos
    for i, example_func in enumerate(examples, 1):
        try:
            example_func()
        except Exception as e:
            print(f"‚ùå Erro no exemplo {i}: {e}")
            import traceback
            traceback.print_exc()
    
    print("\nüéâ Todos os exemplos executados!")
    print("\nüí° Dicas para uso:")
    print("  - Use create_model('2b') para in√≠cio r√°pido")
    print("  - Configure dropout=0.0 para infer√™ncia")
    print("  - Use torch_dtype='float16' para economizar mem√≥ria")
    print("  - Ative use_kv_cache=True para gera√ß√£o sequencial")
    print("  - Monitore uso de mem√≥ria com model.get_memory_footprint()")


if __name__ == "__main__":
    main()