#!/usr/bin/env python3
"""
Demo de Chat para libervia-ia-Core v1
AVISO: Este √© um demo simulado - o modelo N√ÉO foi treinado!
"""

import sys
import time
import random
from pathlib import Path
from typing import List, Dict

import torch
import torch.nn.functional as F

# Adicionar o package ao path
sys.path.insert(0, str(Path(__file__).parent.parent / "packages" / "core"))

from libervia_core import LiberviaConfig, LiberviaForCausalLM


class DummyTokenizer:
    """
    Tokenizer simulado para demo.
    NOTA: Um tokenizer real usaria SentencePiece ou BPE.
    """
    
    def __init__(self, vocab_size: int = 1000):
        self.vocab_size = vocab_size
        
        # Tokens especiais
        self.special_tokens = {
            "<pad>": 0,
            "<unk>": 1, 
            "<bos>": 2,   # Begin of sequence
            "<eos>": 3,   # End of sequence
            "<user>": 4,  # Usu√°rio
            "<assistant>": 5,  # Assistente
        }
        
        # Vocabul√°rio simulado (palavras comuns)
        self.vocab = {
            # Tokens especiais
            **self.special_tokens,
            
            # Palavras b√°sicas em portugu√™s
            "ol√°": 10, "oi": 11, "bom": 12, "dia": 13, "noite": 14,
            "como": 15, "voc√™": 16, "est√°": 17, "tudo": 18, "bem": 19,
            "obrigado": 20, "obrigada": 21, "por": 22, "favor": 23,
            "sim": 24, "n√£o": 25, "talvez": 26, "claro": 27,
            "eu": 28, "sou": 29, "um": 30, "uma": 31, "modelo": 32,
            "de": 33, "linguagem": 34, "artificial": 35, "inteligente": 36,
            "help": 37, "ajuda": 38, "ajudar": 39, "resposta": 40,
            "pergunta": 41, "quest√£o": 42, "d√∫vida": 43,
            "legal": 44, "interessante": 45, "incr√≠vel": 46,
            "python": 47, "c√≥digo": 48, "programa√ß√£o": 49,
            "libervia": 50, "ia": 51, "core": 52,
        }
        
        # Criar vocabul√°rio reverso
        self.id_to_token = {v: k for k, v in self.vocab.items()}
        
        # Preencher resto com tokens dummy
        for i in range(len(self.vocab), vocab_size):
            self.id_to_token[i] = f"<unk_{i}>"
    
    def encode(self, text: str) -> List[int]:
        """Converte texto para tokens (muito simplificado)."""
        # Normalizar texto
        text = text.lower().strip()
        
        # Dividir em palavras
        words = text.split()
        
        # Converter para IDs
        token_ids = [self.special_tokens["<bos>"]]
        
        for word in words:
            # Remover pontua√ß√£o b√°sica
            word = word.strip(".,!?;:")
            
            if word in self.vocab:
                token_ids.append(self.vocab[word])
            else:
                # Token desconhecido - usar hash simples
                token_id = hash(word) % (self.vocab_size - 100) + 100
                token_ids.append(token_id)
        
        token_ids.append(self.special_tokens["<eos>"])
        return token_ids
    
    def decode(self, token_ids: List[int]) -> str:
        """Converte tokens para texto."""
        words = []
        
        for token_id in token_ids:
            if token_id in [0, 2, 3]:  # Skip special tokens
                continue
            
            if token_id in self.id_to_token:
                word = self.id_to_token[token_id]
                if not word.startswith("<"):
                    words.append(word)
            else:
                words.append(f"<{token_id}>")
        
        return " ".join(words)


class LiberviaChat:
    """
    Sistema de chat simulado para libervia-ia.
    AVISO: Modelo n√£o treinado - respostas s√£o aleat√≥rias!
    """
    
    def __init__(self, model_size: str = "tiny"):
        print("ü§ñ Inicializando libervia-ia Chat...")
        
        # Configura√ß√£o ultra pequena para demo
        self.config = LiberviaConfig(
            model_name=f"libervia-chat-{model_size}",
            d_model=128,      # Muito pequeno para demo
            n_layers=2,       # 2 camadas
            n_heads=4,        # 4 cabe√ßas
            num_key_value_heads=4,
            ffn_mult=2.0,
            vocab_size=1000,  # Vocabul√°rio pequeno
            max_sequence_length=128,
            torch_dtype="float32",
            use_flash_attention=False,
            dropout=0.0,
        )
        
        # Criar modelo
        self.model = LiberviaForCausalLM(self.config)
        self.model.eval()
        
        # Tokenizer simulado
        self.tokenizer = DummyTokenizer(self.config.vocab_size)
        
        # Par√¢metros de gera√ß√£o
        self.generation_config = {
            "max_new_tokens": 20,
            "temperature": 0.8,
            "top_p": 0.9,
            "do_sample": True,
        }
        
        print(f"‚úÖ Modelo carregado: {self.model.num_parameters():,} par√¢metros")
        print("‚ö†Ô∏è  AVISO: Modelo N√ÉO foi treinado - respostas s√£o simuladas!")
    
    def format_prompt(self, user_message: str, chat_history: List[Dict] = None) -> str:
        """Formata prompt para conversa."""
        prompt = ""
        
        # Adicionar hist√≥rico se fornecido
        if chat_history:
            for msg in chat_history[-3:]:  # √öltimas 3 mensagens
                role = msg["role"]
                content = msg["content"]
                prompt += f"<{role}> {content} "
        
        # Adicionar mensagem atual
        prompt += f"<user> {user_message} <assistant>"
        
        return prompt
    
    def generate_response(self, prompt: str) -> str:
        """Gera resposta do modelo."""
        try:
            # Tokenizar prompt
            input_ids = self.tokenizer.encode(prompt)
            input_tensor = torch.tensor([input_ids])
            
            print(f"üî§ Prompt tokenizado: {len(input_ids)} tokens")
            
            # Gerar resposta
            generated_ids = input_tensor.clone()
            
            with torch.no_grad():
                for i in range(self.generation_config["max_new_tokens"]):
                    # Forward pass
                    outputs = self.model(generated_ids)
                    logits = outputs['logits'] if isinstance(outputs, dict) else outputs[0]
                    
                    # Logits do √∫ltimo token
                    next_token_logits = logits[0, -1, :] / self.generation_config["temperature"]
                    
                    # Sampling
                    if self.generation_config["do_sample"]:
                        # Top-p sampling
                        probs = F.softmax(next_token_logits, dim=-1)
                        next_token = torch.multinomial(probs, num_samples=1)
                    else:
                        # Greedy
                        next_token = torch.argmax(next_token_logits)
                    
                    # Verificar se √© EOS
                    if next_token.item() == self.tokenizer.special_tokens["<eos>"]:
                        break
                    
                    # Adicionar √† sequ√™ncia
                    generated_ids = torch.cat([generated_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1)
            
            # Extrair apenas os novos tokens (resposta)
            response_ids = generated_ids[0, len(input_ids):].tolist()
            response_text = self.tokenizer.decode(response_ids)
            
            return response_text.strip()
            
        except Exception as e:
            print(f"‚ùå Erro na gera√ß√£o: {e}")
            return self._fallback_response()
    
    def _fallback_response(self) -> str:
        """Resposta de fallback quando h√° erro."""
        responses = [
            "Interessante! Me conte mais sobre isso.",
            "Entendi. Como posso ajudar?",
            "Essa √© uma boa pergunta.",
            "Vou pensar sobre isso.",
            "Posso ajudar com mais alguma coisa?",
            "Obrigado por compartilhar isso comigo.",
            "Como voc√™ est√° se sentindo hoje?",
            "Que legal! Continue.",
        ]
        return random.choice(responses)
    
    def chat(self):
        """Interface de chat interativo."""
        print("\n" + "="*50)
        print("üåü libervia-ia Chat Demo")
        print("="*50)
        print("‚ÑπÔ∏è  Digite 'sair' para terminar")
        print("‚ÑπÔ∏è  Digite 'ajuda' para comandos")
        print("‚ö†Ô∏è  LEMBRE-SE: Modelo n√£o foi treinado!")
        print()
        
        chat_history = []
        
        while True:
            try:
                # Input do usu√°rio
                user_input = input("üë§ Voc√™: ").strip()
                
                if not user_input:
                    continue
                
                # Comandos especiais
                if user_input.lower() in ['sair', 'exit', 'quit']:
                    print("üëã Tchau! Obrigado por testar o libervia-ia!")
                    break
                
                if user_input.lower() in ['ajuda', 'help']:
                    print("üìã Comandos dispon√≠veis:")
                    print("  - 'sair': Terminar chat")
                    print("  - 'limpar': Limpar hist√≥rico")
                    print("  - 'status': Mostrar status do modelo")
                    continue
                
                if user_input.lower() in ['limpar', 'clear']:
                    chat_history = []
                    print("üßπ Hist√≥rico limpo!")
                    continue
                
                if user_input.lower() in ['status']:
                    print(f"üìä Status do modelo:")
                    print(f"   Par√¢metros: {self.model.num_parameters():,}")
                    print(f"   Configura√ß√£o: {self.config.d_model}d √ó {self.config.n_layers}L")
                    print(f"   Hist√≥rico: {len(chat_history)} mensagens")
                    continue
                
                # Adicionar ao hist√≥rico
                chat_history.append({"role": "user", "content": user_input})
                
                # Formatar prompt
                prompt = self.format_prompt(user_input, chat_history)
                
                # Gerar resposta
                print("ü§ñ libervia-ia: ", end="", flush=True)
                
                start_time = time.time()
                response = self.generate_response(prompt)
                end_time = time.time()
                
                print(response)
                print(f"‚è±Ô∏è  ({end_time - start_time:.2f}s)")
                
                # Adicionar resposta ao hist√≥rico
                chat_history.append({"role": "assistant", "content": response})
                
                # Limitar hist√≥rico
                if len(chat_history) > 10:
                    chat_history = chat_history[-8:]  # Manter √∫ltimas 8 mensagens
                
            except KeyboardInterrupt:
                print("\nüëã Chat interrompido. Tchau!")
                break
            except Exception as e:
                print(f"\n‚ùå Erro no chat: {e}")
                continue


def main():
    """Fun√ß√£o principal do chat demo."""
    print("üöÄ libervia-ia Chat Demo")
    print("=" * 30)
    
    try:
        # Criar sistema de chat
        chat_system = LiberviaChat("tiny")
        
        # Iniciar chat interativo
        chat_system.chat()
        
    except Exception as e:
        print(f"‚ùå Erro ao inicializar chat: {e}")
        print("\nüí° Certifique-se de que o libervia-core est√° funcionando:")
        print("   python scripts/quick_test.py")


if __name__ == "__main__":
    main()