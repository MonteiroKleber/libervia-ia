#!/usr/bin/env python3
"""
Script de teste completo para libervia-ia-Core v1
Verifica se todas as funcionalidades est√£o funcionando corretamente
"""

import sys
import os
import time
import traceback
from pathlib import Path

import torch
import torch.nn.functional as F

# Adicionar o package ao path
sys.path.insert(0, str(Path(__file__).parent.parent / "packages" / "core"))

try:
    from libervia_core import (
        LiberviaConfig,
        LiberviaForCausalLM,
        create_model,
        create_config,
        check_flash_attention_availability,
        check_system_compatibility,
        print_system_info,
        print_model_info,
    )
    print("‚úÖ Imports libervia-core OK")
except ImportError as e:
    print(f"‚ùå Erro ao importar libervia-core: {e}")
    sys.exit(1)


def test_basic_functionality():
    """Testa funcionalidades b√°sicas."""
    print("\nüß™ Teste 1: Funcionalidades B√°sicas")
    print("-" * 40)
    
    try:
        # Teste de configura√ß√£o
        config = LiberviaConfig.from_model_size("2b")
        print(f"‚úÖ Config criada: {config.model_name}")
        print(f"   Par√¢metros: {config.total_params:,}")
        print(f"   Mem√≥ria: {config.memory_footprint_mb:.1f} MB")
        
        # Teste de modelo
        model = create_model("2b")
        print(f"‚úÖ Modelo criado: {type(model).__name__}")
        print(f"   Par√¢metros reais: {model.num_parameters():,}")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro no teste b√°sico: {e}")
        traceback.print_exc()
        return False


def test_forward_pass():
    """Testa forward pass do modelo."""
    print("\nüß™ Teste 2: Forward Pass")
    print("-" * 40)
    
    try:
        config = create_config("2b")
        model = LiberviaForCausalLM(config)
        
        # Teste com diferentes tamanhos
        test_cases = [
            (1, 1),    # M√≠nimo
            (1, 10),   # Pequeno
            (2, 20),   # Batch
            (1, 100),  # Sequ√™ncia maior
        ]
        
        for batch_size, seq_len in test_cases:
            input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
            
            start_time = time.time()
            with torch.no_grad():
                outputs = model(input_ids)
                
            end_time = time.time()
            
            if isinstance(outputs, dict):
                logits = outputs['logits']
            else:
                logits = outputs[0]
            
            expected_shape = (batch_size, seq_len, config.vocab_size)
            assert logits.shape == expected_shape, f"Shape incorreta: {logits.shape} vs {expected_shape}"
            
            tokens_per_sec = (batch_size * seq_len) / (end_time - start_time)
            print(f"‚úÖ Batch {batch_size}x{seq_len}: {logits.shape} - {tokens_per_sec:.1f} tok/s")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro no forward pass: {e}")
        traceback.print_exc()
        return False


def test_generation():
    """Testa gera√ß√£o de texto simples."""
    print("\nüß™ Teste 3: Gera√ß√£o de Texto")
    print("-" * 40)
    
    try:
        config = create_config("2b")
        model = LiberviaForCausalLM(config)
        model.eval()
        
        # Prompt inicial
        prompt_ids = torch.randint(0, config.vocab_size, (1, 5))
        print(f"‚úÖ Prompt inicial: {prompt_ids.tolist()}")
        
        # Gera√ß√£o simples (sem sampling sofisticado)
        generated_ids = prompt_ids.clone()
        max_new_tokens = 10
        
        with torch.no_grad():
            for i in range(max_new_tokens):
                # Forward pass
                outputs = model(generated_ids)
                if isinstance(outputs, dict):
                    logits = outputs['logits']
                else:
                    logits = outputs[0]
                
                # Pr√≥ximo token (greedy)
                next_token_logits = logits[0, -1, :]
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                
                # Adicionar √† sequ√™ncia
                generated_ids = torch.cat([generated_ids, next_token.unsqueeze(0)], dim=1)
        
        print(f"‚úÖ Sequ√™ncia gerada: {generated_ids.tolist()}")
        print(f"   Tokens gerados: {max_new_tokens}")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro na gera√ß√£o: {e}")
        traceback.print_exc()
        return False


def test_kv_cache():
    """Testa KV cache para infer√™ncia eficiente."""
    print("\nüß™ Teste 4: KV Cache")
    print("-" * 40)
    
    try:
        config = create_config("2b")
        model = LiberviaForCausalLM(config)
        model.eval()
        
        # Primeira passada
        input_ids = torch.randint(0, config.vocab_size, (1, 5))
        
        start_time = time.time()
        with torch.no_grad():
            outputs1 = model(input_ids, use_cache=True)
        time_with_cache_init = time.time() - start_time
        
        if isinstance(outputs1, dict):
            past_key_values = outputs1['past_key_values']
        else:
            past_key_values = outputs1[-1]
        
        print(f"‚úÖ Cache inicial criado: {len(past_key_values)} layers")
        
        # Segunda passada usando cache
        next_token = torch.randint(0, config.vocab_size, (1, 1))
        
        start_time = time.time()
        with torch.no_grad():
            outputs2 = model(
                next_token,
                past_key_values=past_key_values,
                use_cache=True
            )
        time_with_cache = time.time() - start_time
        
        print(f"‚úÖ Infer√™ncia com cache: {time_with_cache:.4f}s")
        print(f"   Speedup potencial: {time_with_cache_init/time_with_cache:.1f}x")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro no KV cache: {e}")
        traceback.print_exc()
        return False


def test_different_models():
    """Testa diferentes tamanhos de modelo."""
    print("\nüß™ Teste 5: Diferentes Tamanhos")
    print("-" * 40)
    
    model_sizes = ["2b", "4b", "7b"]
    
    for size in model_sizes:
        try:
            config = create_config(size)
            
            # Criar modelo pequeno para teste (reduzir para economizar mem√≥ria)
            config = config.update(
                d_model=min(config.d_model, 512),
                n_layers=min(config.n_layers, 2),
                max_sequence_length=min(config.max_sequence_length, 64)
            )
            
            model = LiberviaForCausalLM(config)
            
            # Teste r√°pido
            input_ids = torch.randint(0, config.vocab_size, (1, 10))
            with torch.no_grad():
                outputs = model(input_ids)
            
            if isinstance(outputs, dict):
                logits = outputs['logits']
            else:
                logits = outputs[0]
            
            print(f"‚úÖ {size.upper()}: {model.num_parameters():,} params - {logits.shape}")
            
        except Exception as e:
            print(f"‚ùå Erro no modelo {size}: {e}")
            return False
    
    return True


def test_memory_usage():
    """Testa uso de mem√≥ria."""
    print("\nüß™ Teste 6: Uso de Mem√≥ria")
    print("-" * 40)
    
    try:
        import psutil
        process = psutil.Process()
        
        # Mem√≥ria inicial
        mem_before = process.memory_info().rss / 1024 / 1024  # MB
        
        # Criar modelo
        config = create_config("2b")
        # Reduzir para teste
        config = config.update(d_model=256, n_layers=2)
        model = LiberviaForCausalLM(config)
        
        # Mem√≥ria ap√≥s cria√ß√£o
        mem_after = process.memory_info().rss / 1024 / 1024  # MB
        model_memory = mem_after - mem_before
        
        # Teste forward
        input_ids = torch.randint(0, config.vocab_size, (1, 10))
        with torch.no_grad():
            outputs = model(input_ids)
        
        # Mem√≥ria ap√≥s forward
        mem_forward = process.memory_info().rss / 1024 / 1024  # MB
        
        print(f"‚úÖ Mem√≥ria baseline: {mem_before:.1f} MB")
        print(f"   Mem√≥ria modelo: +{model_memory:.1f} MB")
        print(f"   Mem√≥ria forward: +{mem_forward - mem_after:.1f} MB")
        print(f"   Total: {mem_forward:.1f} MB")
        
        return True
    except ImportError:
        print("‚ö†Ô∏è  psutil n√£o dispon√≠vel, pulando teste de mem√≥ria")
        return True
    except Exception as e:
        print(f"‚ùå Erro no teste de mem√≥ria: {e}")
        return False


def test_cuda_if_available():
    """Testa CUDA se dispon√≠vel."""
    print("\nüß™ Teste 7: CUDA (se dispon√≠vel)")
    print("-" * 40)
    
    if not torch.cuda.is_available():
        print("‚ö†Ô∏è  CUDA n√£o dispon√≠vel, pulando teste")
        return True
    
    try:
        config = create_config("2b")
        # Modelo pequeno para CUDA
        config = config.update(d_model=256, n_layers=2)
        
        model = LiberviaForCausalLM(config).cuda()
        input_ids = torch.randint(0, config.vocab_size, (1, 10)).cuda()
        
        start_time = time.time()
        with torch.no_grad():
            outputs = model(input_ids)
        cuda_time = time.time() - start_time
        
        if isinstance(outputs, dict):
            logits = outputs['logits']
        else:
            logits = outputs[0]
        
        assert logits.is_cuda, "Output n√£o est√° em CUDA"
        
        print(f"‚úÖ CUDA OK: {cuda_time:.4f}s")
        print(f"   GPU: {torch.cuda.get_device_name()}")
        print(f"   Mem√≥ria: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro no teste CUDA: {e}")
        traceback.print_exc()
        return False


def benchmark_performance():
    """Benchmark b√°sico de performance."""
    print("\nüß™ Benchmark: Performance")
    print("-" * 40)
    
    try:
        config = create_config("2b")
        # Modelo otimizado para benchmark
        config = config.update(d_model=512, n_layers=4)
        model = LiberviaForCausalLM(config)
        model.eval()
        
        # Diferentes cen√°rios
        scenarios = [
            ("Batch=1, Seq=1", 1, 1),
            ("Batch=1, Seq=10", 1, 10),
            ("Batch=1, Seq=100", 1, 100),
            ("Batch=4, Seq=10", 4, 10),
        ]
        
        print(f"{'Cen√°rio':<20} {'Tempo (ms)':<12} {'Tok/s':<10} {'Throughput':<15}")
        print("-" * 65)
        
        for name, batch_size, seq_len in scenarios:
            input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
            
            # Warmup
            with torch.no_grad():
                model(input_ids)
            
            # Benchmark
            times = []
            for _ in range(10):
                start_time = time.time()
                with torch.no_grad():
                    outputs = model(input_ids)
                end_time = time.time()
                times.append(end_time - start_time)
            
            avg_time = sum(times) / len(times)
            total_tokens = batch_size * seq_len
            tokens_per_sec = total_tokens / avg_time
            
            print(f"{name:<20} {avg_time*1000:<12.1f} {tokens_per_sec:<10.1f} {total_tokens/avg_time:<15.1f}")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro no benchmark: {e}")
        return False


def main():
    """Fun√ß√£o principal de teste."""
    print("üöÄ libervia-ia-Core v1 - Teste Completo")
    print("=" * 50)
    
    # Informa√ß√µes do sistema
    print_system_info()
    print()
    
    # Lista de testes
    tests = [
        ("Funcionalidades B√°sicas", test_basic_functionality),
        ("Forward Pass", test_forward_pass),
        ("Gera√ß√£o de Texto", test_generation),
        ("KV Cache", test_kv_cache),
        ("Diferentes Modelos", test_different_models),
        ("Uso de Mem√≥ria", test_memory_usage),
        ("CUDA", test_cuda_if_available),
    ]
    
    # Executar testes
    results = []
    total_start = time.time()
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        start_time = time.time()
        
        try:
            success = test_func()
            end_time = time.time()
            
            if success:
                print(f"‚úÖ {test_name}: PASSOU ({end_time - start_time:.2f}s)")
                results.append((test_name, True, end_time - start_time))
            else:
                print(f"‚ùå {test_name}: FALHOU ({end_time - start_time:.2f}s)")
                results.append((test_name, False, end_time - start_time))
                
        except Exception as e:
            end_time = time.time()
            print(f"üí• {test_name}: ERRO - {e}")
            results.append((test_name, False, end_time - start_time))
    
    # Benchmark opcional
    print(f"\n{'='*20} Benchmark Opcional {'='*20}")
    try:
        benchmark_performance()
    except Exception as e:
        print(f"‚ö†Ô∏è  Benchmark falhou: {e}")
    
    total_time = time.time() - total_start
    
    # Relat√≥rio final
    print(f"\nüèÅ RELAT√ìRIO FINAL")
    print("=" * 50)
    
    passed = sum(1 for _, success, _ in results if success)
    total = len(results)
    
    print(f"Testes executados: {total}")
    print(f"Testes passou: {passed}")
    print(f"Testes falhou: {total - passed}")
    print(f"Taxa de sucesso: {passed/total*100:.1f}%")
    print(f"Tempo total: {total_time:.2f}s")
    
    print("\nDetalhes:")
    for test_name, success, test_time in results:
        status = "‚úÖ PASSOU" if success else "‚ùå FALHOU"
        print(f"  {test_name:<25} {status} ({test_time:.2f}s)")
    
    if passed == total:
        print(f"\nüéâ TODOS OS TESTES PASSARAM!")
        print("libervia-ia-Core est√° funcionando corretamente!")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} teste(s) falharam.")
        print("Verifique os erros acima.")
        return 1


if __name__ == "__main__":
    sys.exit(main())